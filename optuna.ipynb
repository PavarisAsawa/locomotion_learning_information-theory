{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ceca9e3f",
   "metadata": {},
   "source": [
    "# Optuna Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "330d9d97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/workstation/pavaris_ws/locomotion_learning_information-theory/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader , TensorDataset\n",
    "from torch.optim import Adam\n",
    "import torch.nn.init as init\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from matplotlib.ticker import MultipleLocator\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "import copy\n",
    "import seaborn as sns\n",
    "\n",
    "from scipy.stats import norm\n",
    "from sklearn.neighbors import KernelDensity, LocalOutlierFactor\n",
    "\n",
    "import tqdm\n",
    "\n",
    "import optuna\n",
    "from optuna.trial import TrialState\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5627f2",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c29dbfd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model hyperparameters\n",
    "cuda = torch.cuda.is_available()\n",
    "DEVICE = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "\n",
    "num_seeds = 30\n",
    "seed = 0\n",
    "\n",
    "all_state_dim = 64\n",
    "state_dim = 64\n",
    "action_dim = 19\n",
    "training_seed = 21\n",
    "\n",
    "# Load fullstate\n",
    "data_fullstate = np.empty(num_seeds, dtype=object)\n",
    "data_no_joint_pos = np.empty(num_seeds, dtype=object)\n",
    "data_no_joint_vel = np.empty(num_seeds, dtype=object)\n",
    "data_no_action = np.empty(num_seeds, dtype=object)\n",
    "data_no_imu = np.empty(num_seeds, dtype=object)\n",
    "data_no_fc = np.empty(num_seeds, dtype=object)\n",
    "for i in range(num_seeds):\n",
    "    data_fullstate[i] = np.load(f\"data/performance/HEBB-FULL-STATE_seed-{seed}-fullstate-rand-{i}.npz\")    \n",
    "    data_no_joint_pos[i] = np.load(f\"data/performance/HEBB-FULL-STATE_seed-{seed}-no_joint_pos-rand-{i}.npz\")\n",
    "    data_no_joint_vel[i] = np.load(f\"data/performance/HEBB-FULL-STATE_seed-{seed}-no_joint_vel-rand-{i}.npz\")\n",
    "    data_no_action[i] = np.load(f\"data/performance/HEBB-FULL-STATE_seed-{seed}-no_action-rand-{i}.npz\")\n",
    "    data_no_imu[i] = np.load(f\"data/performance/HEBB-FULL-STATE_seed-{seed}-no_imu-rand-{i}.npz\")\n",
    "    data_no_fc[i] = np.load(f\"data/performance/HEBB-FULL-STATE_seed-{seed}-no_fc-rand-{i}.npz\")\n",
    "    \n",
    "train_x = torch.empty((0, all_state_dim), dtype=torch.float32 ,device=DEVICE)\n",
    "train_y = torch.empty((0, action_dim), dtype=torch.float32,device=DEVICE)\n",
    "test_x = torch.empty((0, all_state_dim), dtype=torch.float32,device=DEVICE)\n",
    "test_y = torch.empty((0, action_dim), dtype=torch.float32,device=DEVICE)\n",
    "for i in range(training_seed):\n",
    "    train_x = torch.cat((train_x, torch.tensor(data_fullstate[i][\"state\"].reshape(data_fullstate[i][\"state\"].shape[0], -1), dtype=torch.float32,device=DEVICE)), dim=0)\n",
    "    train_y = torch.cat((train_y, torch.tensor(data_fullstate[i][\"action_lowpass\"].reshape(data_fullstate[i][\"action_lowpass\"].shape[0], -1), dtype=torch.float32,device=DEVICE)), dim=0)\n",
    "for j in range(training_seed, num_seeds):\n",
    "    test_x = torch.cat((test_x, torch.tensor(data_fullstate[j][\"state\"].reshape(data_no_joint_pos[j][\"state\"].shape[0], -1), dtype=torch.float32,device=DEVICE)), dim=0)\n",
    "    test_y = torch.cat((test_y, torch.tensor(data_fullstate[j][\"action_lowpass\"].reshape(data_no_joint_pos[j][\"action_lowpass\"].shape[0], -1), dtype=torch.float32,device=DEVICE)), dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c6a0f8",
   "metadata": {},
   "source": [
    "## Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a0e828d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    A simple implementation of Gaussian MLP Encoder\n",
    "\"\"\"\n",
    "class Predictor(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(Predictor, self).__init__()\n",
    "\n",
    "        self.FC_input = nn.Linear(input_dim, hidden_dim)\n",
    "        self.FC_input2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        \n",
    "        self.FC_mean  = nn.Linear(hidden_dim, output_dim)\n",
    "        self.FC_var   = nn.Linear (hidden_dim, output_dim)\n",
    "        \n",
    "        self.LeakyReLU = nn.LeakyReLU(0.2)\n",
    "        self.tanh = nn.Tanh()\n",
    "        # self.LeakyReLU = nn.ReLU()\n",
    "        self.softplus = nn.Softplus()\n",
    "        \n",
    "        self.training = True\n",
    "        \n",
    "    def reparameterization(self , mean, var):\n",
    "        epsilon = torch.randn_like(var).to(DEVICE)        # sampling epsilon        \n",
    "        z = mean + var*epsilon                          # reparameterization trick\n",
    "        return z\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h_       = self.tanh(self.FC_input(x))\n",
    "        h_       = self.tanh(self.FC_input2(h_))\n",
    "        mean     = self.FC_mean(h_)         # encoder produces mean and log of variance \n",
    "        log_var  = self.FC_var(h_)          # (i.e., parateters of simple tractable normal distribution \"q\"\n",
    "        # log_var  = self.softplus(log_var)  # clamp log_var to avoid numerical issues\n",
    "        \n",
    "        z = self.reparameterization(mean, torch.exp(log_var))  # reparameterization trick\n",
    "        # z is sampling from the distribution z = mean + var * epsilon\n",
    "        return z,mean, log_var"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077c824d",
   "metadata": {},
   "source": [
    "## Optuna setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "170932f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN : X , Y shape :  torch.Size([21000, 19]) torch.Size([21000, 19])\n",
      "TEST : X , Y shape :  torch.Size([9000, 19]) torch.Size([9000, 19])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1000\n",
    "\n",
    "state_index = torch.arange(0, 19) \n",
    "state_dim = len(state_index)\n",
    "\n",
    "train_dataset = TensorDataset(train_x[:,state_index], train_y)\n",
    "test_dataset = TensorDataset(test_x[:,state_index], test_y)\n",
    "\n",
    "print(\"TRAIN : X , Y shape : \",train_x[:,state_index].shape , train_y.shape)\n",
    "print(\"TEST : X , Y shape : \",test_x[:,state_index].shape , test_y.shape)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dfb91f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dim = 256\n",
    "input_dim = state_dim\n",
    "output_dim = 19\n",
    "epochs = 150\n",
    "\n",
    "def objective(trial):\n",
    "    \n",
    "    lr = trial.suggest_float(\"lr\", 1e-6, 1e-3, log=True)\n",
    "    hidden_dim = trial.suggest_categorical(\"hidden_dim\", [64, 128,256 ,512, 1024])\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [16 , 32 ,64, 128, 256, 512, 1024, 2048])    \n",
    "\n",
    "    model = Predictor(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim).to(DEVICE)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    optimizer = Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "\n",
    "        for x_batch, y_batch in train_loader:\n",
    "            x_batch = x_batch.to(DEVICE)    # dim = [batch_size, state_dim]\n",
    "            y_batch = y_batch.to(DEVICE)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            pred, mean , log_var = model(x_batch)\n",
    "            loss = criterion(pred, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item() * x_batch.size(0)\n",
    "            \n",
    "\n",
    "        avg_loss = total_loss / len(train_dataset)\n",
    "        trial.report(avg_loss, epoch)\n",
    "        # train_losses.append(avg_loss)\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "973ddc01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch: 2.8.0+cu128\n",
      "torchvision: 0.23.0+cu128\n"
     ]
    }
   ],
   "source": [
    "print(\"torch:\", torch.__version__)\n",
    "try:\n",
    "    import torchvision; print(\"torchvision:\", torchvision.__version__)\n",
    "except Exception as e:\n",
    "    print(\"torchvision: not installed\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c2cd48f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-13 20:54:28,315] A new study created in memory with name: no-name-c4b82a2a-fad2-4ada-a68e-7826f164bdf1\n",
      "[I 2025-08-13 20:55:21,985] Trial 0 finished with value: 0.02293311952693122 and parameters: {'lr': 1.0605487875263822e-06, 'hidden_dim': 512, 'batch_size': 64}. Best is trial 0 with value: 0.02293311952693122.\n",
      "[I 2025-08-13 20:55:56,982] Trial 1 finished with value: 0.013315909003927593 and parameters: {'lr': 8.788217136968343e-05, 'hidden_dim': 1024, 'batch_size': 128}. Best is trial 1 with value: 0.013315909003927593.\n",
      "[I 2025-08-13 20:59:01,672] Trial 2 finished with value: 0.01152472831060489 and parameters: {'lr': 0.0004999422871121817, 'hidden_dim': 256, 'batch_size': 16}. Best is trial 2 with value: 0.01152472831060489.\n",
      "[I 2025-08-13 21:01:59,748] Trial 3 finished with value: 0.012265413585163297 and parameters: {'lr': 0.00038661307637963814, 'hidden_dim': 128, 'batch_size': 16}. Best is trial 2 with value: 0.01152472831060489.\n",
      "[I 2025-08-13 21:02:15,200] Trial 4 finished with value: 0.43221777913683934 and parameters: {'lr': 8.812174094626007e-06, 'hidden_dim': 64, 'batch_size': 1024}. Best is trial 2 with value: 0.01152472831060489.\n",
      "[W 2025-08-13 21:02:48,882] Trial 5 failed with parameters: {'lr': 3.6538908532058296e-05, 'hidden_dim': 256, 'batch_size': 32} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/workstation/pavaris_ws/locomotion_learning_information-theory/.venv/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 201, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_275790/887229480.py\", line 29, in objective\n",
      "    loss.backward()\n",
      "  File \"/home/workstation/pavaris_ws/locomotion_learning_information-theory/.venv/lib/python3.10/site-packages/torch/_tensor.py\", line 647, in backward\n",
      "    torch.autograd.backward(\n",
      "  File \"/home/workstation/pavaris_ws/locomotion_learning_information-theory/.venv/lib/python3.10/site-packages/torch/autograd/__init__.py\", line 354, in backward\n",
      "    _engine_run_backward(\n",
      "  File \"/home/workstation/pavaris_ws/locomotion_learning_information-theory/.venv/lib/python3.10/site-packages/torch/autograd/graph.py\", line 829, in _engine_run_backward\n",
      "    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "KeyboardInterrupt\n",
      "[W 2025-08-13 21:02:48,883] Trial 5 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mminimize\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m900\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m pruned_trials \u001b[38;5;241m=\u001b[39m study\u001b[38;5;241m.\u001b[39mget_trials(deepcopy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, states\u001b[38;5;241m=\u001b[39m[TrialState\u001b[38;5;241m.\u001b[39mPRUNED])\n\u001b[1;32m      5\u001b[0m complete_trials \u001b[38;5;241m=\u001b[39m study\u001b[38;5;241m.\u001b[39mget_trials(deepcopy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, states\u001b[38;5;241m=\u001b[39m[TrialState\u001b[38;5;241m.\u001b[39mCOMPLETE])\n",
      "File \u001b[0;32m~/pavaris_ws/locomotion_learning_information-theory/.venv/lib/python3.10/site-packages/optuna/study/study.py:489\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21moptimize\u001b[39m(\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    389\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    396\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    397\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    398\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    399\u001b[0m \n\u001b[1;32m    400\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    487\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    488\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 489\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/pavaris_ws/locomotion_learning_information-theory/.venv/lib/python3.10/site-packages/optuna/study/_optimize.py:64\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 64\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     77\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/pavaris_ws/locomotion_learning_information-theory/.venv/lib/python3.10/site-packages/optuna/study/_optimize.py:161\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 161\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[0;32m~/pavaris_ws/locomotion_learning_information-theory/.venv/lib/python3.10/site-packages/optuna/study/_optimize.py:253\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    249\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[1;32m    250\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[1;32m    252\u001b[0m ):\n\u001b[0;32m--> 253\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[0;32m~/pavaris_ws/locomotion_learning_information-theory/.venv/lib/python3.10/site-packages/optuna/study/_optimize.py:201\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 201\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    203\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    204\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[0;32mIn[6], line 29\u001b[0m, in \u001b[0;36mobjective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m     27\u001b[0m pred, mean , log_var \u001b[38;5;241m=\u001b[39m model(x_batch)\n\u001b[1;32m     28\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(pred, y_batch)\n\u001b[0;32m---> 29\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     32\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m*\u001b[39m x_batch\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/pavaris_ws/locomotion_learning_information-theory/.venv/lib/python3.10/site-packages/torch/_tensor.py:647\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    637\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    638\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    639\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    640\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    645\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    646\u001b[0m     )\n\u001b[0;32m--> 647\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    648\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    649\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/pavaris_ws/locomotion_learning_information-theory/.venv/lib/python3.10/site-packages/torch/autograd/__init__.py:354\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    349\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    351\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 354\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    362\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/pavaris_ws/locomotion_learning_information-theory/.venv/lib/python3.10/site-packages/torch/autograd/graph.py:829\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    827\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 829\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    830\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    831\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    833\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=100, timeout=900)\n",
    "\n",
    "pruned_trials = study.get_trials(deepcopy=False, states=[TrialState.PRUNED])\n",
    "complete_trials = study.get_trials(deepcopy=False, states=[TrialState.COMPLETE])\n",
    "\n",
    "print(\"Study statistics: \")\n",
    "print(\"  Number of finished trials: \", len(study.trials))\n",
    "print(\"  Number of pruned trials: \", len(pruned_trials))\n",
    "print(\"  Number of complete trials: \", len(complete_trials))\n",
    "\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "\n",
    "print(\"  Value: \", trial.value)\n",
    "\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8daaf938",
   "metadata": {},
   "source": [
    "# Use Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175f82eb",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3988b65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "eva_input = train_x[:500 , :19]\n",
    "eva_label = train_y[:500 , :]\n",
    "\n",
    "eva_input = test_x[:500 , :19]\n",
    "eva_label = test_y[:500 , :]\n",
    "\n",
    "eva_pred , eva_mean , eva_log_var = model(eva_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d76151",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim  = 19\n",
    "cols = 3\n",
    "rows = math.ceil(dim / cols)\n",
    "\n",
    "# ตั้งค่าสถานะว่า eva_sigma เป็น std หรือ var\n",
    "ASSUME_SIGMA_IS_STD = True  # ถ้า eva_sigma คือ std ให้ True, ถ้าเป็น variance อยู่แล้ว ให้ False\n",
    "\n",
    "# เตรียมแกน x (จะพล็อตตามลำดับ index ของ sample)\n",
    "T = eva_mean.shape[0]\n",
    "x = np.arange(T)\n",
    "\n",
    "# แปลงเป็น numpy\n",
    "y_np = eva_label.detach().cpu().numpy()\n",
    "pred_np = eva_pred.detach().cpu().numpy()\n",
    "mu_np  = eva_mean.detach().cpu().numpy()                   # [T, 19]\n",
    "sig_np = torch.exp(eva_log_var).detach().cpu().numpy()                  # [T, 19]\n",
    "var_np = (sig_np**2) if ASSUME_SIGMA_IS_STD else sig_np    # [T, 19]\n",
    "# var_np = np.clip(var_np, 1e-12, None)                      # กันติดลบ/ศูนย์"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67661c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# เริ่มวาด 19 subplot, 3 columns\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(cols*5, rows*3), sharex=True)\n",
    "axes = axes.flatten()\n",
    "\n",
    "for d in range(dim):\n",
    "    ax = axes[d]\n",
    "    mu   = mu_np[:, d]\n",
    "    var  = var_np[:, d]\n",
    "    pred = pred_np[:, d]\n",
    "    std  = np.sqrt(var)\n",
    "    lo   = mu - 1.96 * std\n",
    "    up   = mu + 1.96 * std\n",
    "    y    = y_np[:, d]\n",
    "\n",
    "    # เส้น mean\n",
    "    ax.plot(x, pred, label=f\"predicted (dim {d})\")\n",
    "    # ax.plot(x, y, label=f\"label (dim {d})\")\n",
    "    # ax.plot(x, mu, label=f\"mean (dim {d})\")\n",
    "    # แถบ 95% CI\n",
    "    ax.fill_between(x, lo, up, alpha=0.4, label=\"95% CI\")\n",
    "\n",
    "    ax.set_title(f\"Dim {d}\")\n",
    "    ax.grid(True)\n",
    "\n",
    "fig.tight_layout()\n",
    "# ดึง legend จากซับพล็อตแรกเพื่อลดความรก\n",
    "handles, labels = axes[0].get_legend_handles_labels()\n",
    "fig.legend(handles, labels, loc=\"upper center\", ncol=3, bbox_to_anchor=(0.5, 1.04))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
