{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ceca9e3f",
   "metadata": {},
   "source": [
    "# Optuna Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "330d9d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader , TensorDataset\n",
    "from torch.optim import Adam\n",
    "import torch.nn.init as init\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from matplotlib.ticker import MultipleLocator\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "import copy\n",
    "import seaborn as sns\n",
    "\n",
    "from scipy.stats import norm\n",
    "from sklearn.neighbors import KernelDensity, LocalOutlierFactor\n",
    "\n",
    "import tqdm\n",
    "\n",
    "import optuna\n",
    "from optuna.trial import TrialState\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5627f2",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c29dbfd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model hyperparameters\n",
    "cuda = torch.cuda.is_available()\n",
    "DEVICE = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "\n",
    "num_seeds = 300\n",
    "seed = 0\n",
    "\n",
    "all_state_dim = 64\n",
    "state_dim = 64\n",
    "action_dim = 19\n",
    "training_seed = 300\n",
    "\n",
    "# Load fullstate\n",
    "data_fullstate = np.empty(num_seeds, dtype=object)\n",
    "data_no_joint_pos = np.empty(num_seeds, dtype=object)\n",
    "data_no_joint_vel = np.empty(num_seeds, dtype=object)\n",
    "data_no_action = np.empty(num_seeds, dtype=object)\n",
    "data_no_imu = np.empty(num_seeds, dtype=object)\n",
    "data_no_fc = np.empty(num_seeds, dtype=object)\n",
    "for i in range(num_seeds): # HEBB-FULL_STATE-seed_0-fullstate-rand-0\n",
    "    data_fullstate[i] = np.load(f\"data/HEBB-full/HEBB-FULL_STATE-seed_{seed}-fullstate-rand-{i}.npz\")    \n",
    "    \n",
    "train_x = torch.empty((0, all_state_dim), dtype=torch.float32 ,device=DEVICE)\n",
    "train_y = torch.empty((0, action_dim), dtype=torch.float32,device=DEVICE)\n",
    "test_x = torch.empty((0, all_state_dim), dtype=torch.float32,device=DEVICE)\n",
    "test_y = torch.empty((0, action_dim), dtype=torch.float32,device=DEVICE)\n",
    "for i in range(training_seed):\n",
    "    train_x = torch.cat((train_x, torch.tensor(data_fullstate[i][\"state\"].reshape(data_fullstate[i][\"state\"].shape[0], -1), dtype=torch.float32,device=DEVICE)), dim=0)\n",
    "    train_y = torch.cat((train_y, torch.tensor(data_fullstate[i][\"action_lowpass\"].reshape(data_fullstate[i][\"action_lowpass\"].shape[0], -1), dtype=torch.float32,device=DEVICE)), dim=0)\n",
    "# for j in range(training_seed, num_seeds):\n",
    "#     test_x = torch.cat((test_x, torch.tensor(data_fullstate[j][\"state\"].reshape(data_fullstate[j][\"state\"].shape[0], -1), dtype=torch.float32,device=DEVICE)), dim=0)\n",
    "#     test_y = torch.cat((test_y, torch.tensor(data_fullstate[j][\"action_lowpass\"].reshape(data_fullstate[j][\"action_lowpass\"].shape[0], -1), dtype=torch.float32,device=DEVICE)), dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c6a0f8",
   "metadata": {},
   "source": [
    "## Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "2a0e828d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    A simple implementation of Gaussian MLP Encoder\n",
    "\"\"\"\n",
    "class Predictor(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(Predictor, self).__init__()\n",
    "\n",
    "        self.FC_input = nn.Linear(input_dim, hidden_dim)\n",
    "        self.FC_input2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        \n",
    "        self.FC_mean  = nn.Linear(hidden_dim, output_dim)\n",
    "        self.FC_var   = nn.Linear (hidden_dim, output_dim)\n",
    "        \n",
    "        self.LeakyReLU = nn.LeakyReLU(0.2)\n",
    "        self.tanh = nn.Tanh()\n",
    "        # self.LeakyReLU = nn.ReLU()\n",
    "        self.softplus = nn.Softplus()\n",
    "        \n",
    "        self.training = True\n",
    "        \n",
    "    def reparameterization(self , mean, var):\n",
    "        epsilon = torch.randn_like(var).to(DEVICE)        # sampling epsilon        \n",
    "        z = mean + var*epsilon                          # reparameterization trick\n",
    "        return z\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h_       = self.tanh(self.FC_input(x))\n",
    "        h_       = self.tanh(self.FC_input2(h_))\n",
    "        mean     = self.FC_mean(h_)         # encoder produces mean and log of variance \n",
    "        log_var  = self.FC_var(h_)          # (i.e., parateters of simple tractable normal distribution \"q\"\n",
    "        # log_var  = self.softplus(log_var)  # clamp log_var to avoid numerical issues\n",
    "        \n",
    "        z = self.reparameterization(mean, torch.exp(log_var))  # reparameterization trick\n",
    "        # z is sampling from the distribution z = mean + var * epsilon\n",
    "        return z,mean, log_var"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077c824d",
   "metadata": {},
   "source": [
    "## Optuna setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "170932f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "\n",
    "pos_index = torch.arange(0, 19) # pos\n",
    "vel_index = torch.arange(19, 38) # vel\n",
    "action_index = torch.arange(38, 57) # vel\n",
    "IMU_index = torch.arange(57, 60)\n",
    "fc_index = torch.arange(60, 64)\n",
    "\n",
    "pos_dataset = TensorDataset(train_x[:,pos_index], train_y)\n",
    "vel_dataset = TensorDataset(train_x[:,vel_index], train_y)\n",
    "action_dataset = TensorDataset(train_x[:,action_index], train_y)\n",
    "IMU_dataset = TensorDataset(train_x[:,IMU_index], train_y)\n",
    "fc_dataset = TensorDataset(train_x[:,fc_index], train_y)\n",
    "\n",
    "# pos_loader = DataLoader(pos_dataset, batch_size=batch_size)\n",
    "# vel_loader = DataLoader(vel_dataset, batch_size=batch_size)\n",
    "# action_loader = DataLoader(action_dataset, batch_size=batch_size)\n",
    "# IMU_loader = DataLoader(IMU_dataset, batch_size=batch_size)\n",
    "# fc_loader = DataLoader(fc_dataset, batch_size=batch_size)\n",
    "\n",
    "# print(\"TRAIN : X , Y shape : \",train_x[:,state_index].shape , train_y.shape)\n",
    "# print(\"TEST : X , Y shape : \",test_x[:,state_index].shape , test_y.sha/pe)\n",
    "\n",
    "# test_dataset = TensorDataset(test_x[:,state_index], test_y)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "dfb91f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dim = 256\n",
    "input_dim = 4\n",
    "output_dim = 19\n",
    "epochs = 150\n",
    "\n",
    "train_dataset = fc_dataset\n",
    "\n",
    "def objective(trial):\n",
    "    \n",
    "    lr = trial.suggest_float(\"lr\", 1e-6, 1e-3, log=True)\n",
    "    hidden_dim = trial.suggest_categorical(\"hidden_dim\", [128, 256 ,512, 1024])\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [16 , 32 ,64, 128, 256, 512, 1024, 2048])    \n",
    "\n",
    "    model = Predictor(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim).to(DEVICE)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    optimizer = Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        count = 0\n",
    "        for x_batch, y_batch in train_loader:\n",
    "            x_batch = x_batch.to(DEVICE)    # dim = [batch_size, state_dim]\n",
    "            y_batch = y_batch.to(DEVICE)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            pred, mean , log_var = model(x_batch)\n",
    "            loss = criterion(pred, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item() * x_batch.size(0)\n",
    "            count += 1\n",
    "\n",
    "        avg_loss = total_loss / len(train_dataset)\n",
    "        trial.report(avg_loss, epoch)\n",
    "        # train_losses.append(avg_loss)\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c2cd48f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-16 14:48:03,319] A new study created in memory with name: no-name-ca66b019-0fac-4af7-a571-69cae407aeae\n",
      "[I 2025-08-16 14:54:24,875] Trial 0 finished with value: 0.09381456708351771 and parameters: {'lr': 5.583649229448393e-06, 'hidden_dim': 512, 'batch_size': 64}. Best is trial 0 with value: 0.09381456708351771.\n",
      "[I 2025-08-16 14:56:24,303] Trial 1 finished with value: 0.09388205257018407 and parameters: {'lr': 4.078409024963005e-06, 'hidden_dim': 1024, 'batch_size': 1024}. Best is trial 0 with value: 0.09381456708351771.\n",
      "[I 2025-08-16 14:58:18,520] Trial 2 finished with value: 0.09383780233939489 and parameters: {'lr': 0.00013406276551057106, 'hidden_dim': 128, 'batch_size': 512}. Best is trial 0 with value: 0.09381456708351771.\n",
      "[I 2025-08-16 15:02:11,428] Trial 3 finished with value: 0.09391831790447235 and parameters: {'lr': 0.0005518345317866675, 'hidden_dim': 1024, 'batch_size': 128}. Best is trial 0 with value: 0.09381456708351771.\n",
      "[I 2025-08-16 15:23:17,074] Trial 4 finished with value: 0.09380147417028745 and parameters: {'lr': 1.1236751914030148e-05, 'hidden_dim': 128, 'batch_size': 16}. Best is trial 4 with value: 0.09380147417028745.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Study statistics: \n",
      "  Number of finished trials:  5\n",
      "  Number of pruned trials:  0\n",
      "  Number of complete trials:  5\n",
      "Best trial:\n",
      "  Value:  0.09380147417028745\n",
      "  Params: \n",
      "    lr: 1.1236751914030148e-05\n",
      "    hidden_dim: 128\n",
      "    batch_size: 16\n"
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=100, timeout=900)\n",
    "\n",
    "pruned_trials = study.get_trials(deepcopy=False, states=[TrialState.PRUNED])\n",
    "complete_trials = study.get_trials(deepcopy=False, states=[TrialState.COMPLETE])\n",
    "\n",
    "print(\"Study statistics: \")\n",
    "print(\"  Number of finished trials: \", len(study.trials))\n",
    "print(\"  Number of pruned trials: \", len(pruned_trials))\n",
    "print(\"  Number of complete trials: \", len(complete_trials))\n",
    "\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "\n",
    "print(\"  Value: \", trial.value)\n",
    "\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8daaf938",
   "metadata": {},
   "source": [
    "# Use Model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
